{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre: Ulises Gallardo Rodr√≠guez\n",
    "\n",
    "Tarea #3 Segundo Parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "    \n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for tuit in f_corpus:\n",
    "            tr_txt += [tuit]\n",
    "            \n",
    "        for label in f_truth:\n",
    "            tr_y += [label]\n",
    "            \n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"./mex_train.txt\", \"./mex_train_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5544  -  5544\n"
     ]
    }
   ],
   "source": [
    "print(len(tr_txt), \" - \", len(tr_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tr_txt)):\n",
    "    tr_txt[i]= tr_txt[i].lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(tr_txt, indices_palabras, n): #bigramas, fase 1\n",
    "    indices_palabras[\"<s>\"] = len(indices_palabras)+1\n",
    "    indices_palabras[\"</s>\"] = len(indices_palabras)+1\n",
    "    indices_palabras[\"<UNK>\"] = len(indices_palabras)+1\n",
    "    n+=3 \n",
    "    Probablilities = np.zeros((n,n), dtype = \"int\")\n",
    "    #para el unigrama usar fdist = nltk.FreqDist(corpus_palabras)\n",
    "    \n",
    "    for i in range(len(tr_txt)):\n",
    "        tweet = tr_txt[i]\n",
    "        tweet = \"<s>\"+tweet+\"</s>\" #bigramas\n",
    "        words = tokenizer.tokenize(tweet)\n",
    "        #print(words)\n",
    "        \n",
    "        for index, word in enumerate(words[1:]):\n",
    "            #print(words[index], word)\n",
    "            y = indices_palabras[word]\n",
    "            x = indices_palabras[words[index]] #prev\n",
    "            Probablilities[y][x]+=1\n",
    "    return Probablilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_prueba, tr_y_prueba = get_texts_from_file(\"./prueba.txt\", \"./prueba.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_palabras = []\n",
    "for doc in tr_prueba:\n",
    "    corpus_palabras += tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'Sam',\n",
       " 'Sam',\n",
       " 'I',\n",
       " 'am',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'like',\n",
       " 'green',\n",
       " 'eggs',\n",
       " 'and',\n",
       " 'ham']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_palabras = dict()\n",
    "cnt = 0\n",
    "V = set(corpus_palabras)\n",
    "for word in V:\n",
    "    indices_palabras[word]=cnt\n",
    "    cnt += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0,\n",
       " 'like': 1,\n",
       " 'eggs': 2,\n",
       " 'I': 3,\n",
       " 'and': 4,\n",
       " 'ham': 5,\n",
       " 'do': 6,\n",
       " 'green': 7,\n",
       " 'Sam': 8,\n",
       " 'not': 9}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probablilities = n_grams(tr_prueba, indices_palabras, len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Probablilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_palabras = []\n",
    "for doc in tr_txt:\n",
    "    corpus_palabras += tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13581"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus_palabras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_palabras = dict()\n",
    "cont = 0\n",
    "V = set(corpus_palabras)\n",
    "for word in V:\n",
    "    indices_palabras[word]=cont\n",
    "    cont += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probablilities = n_grams(tr_txt, indices_palabras, len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Probablilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
