{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_Model:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.max_ngram = 3\n",
    "        self.Count_unigram = {}\n",
    "        self.Count_bigram = {}\n",
    "        self.Count_trigram = {}\n",
    "        self.n_grams_counts()\n",
    "\n",
    "    def n_grams_counts(self):\n",
    "        Count = {}\n",
    "        start = []\n",
    "        end = [\"</s>\"]\n",
    "        for i in range(max(1,self.max_ngram-1)):\n",
    "            start.append(\"<s>\")\n",
    "\n",
    "        for tweet in self.corpus:\n",
    "            words = start+tweet+end \n",
    "            for index, word in enumerate(words[self.max_ngram-1:]):\n",
    "                context = [word]\n",
    "                if word not in self.Count_unigram: \n",
    "                    self.Count_unigram[word] = 0\n",
    "                self.Count_unigram[word]+=1\n",
    "\n",
    "                for j in range(1,self.max_ngram): #Create n_gram\n",
    "                    context.append(words[(index + self.max_ngram -1) - j])\n",
    "                    if len(context) == 2:\n",
    "                        if self.Count_bigram.get(str(context)) is not None: self.Count_bigram[str(context)] += 1\n",
    "                        else: self.Count_bigram[str(context)] = 1\n",
    "                    elif len(context) == 3:\n",
    "                        if self.Count_trigram.get(str(context)) is not None: self.Count_trigram[str(context)] += 1\n",
    "                        else: self.Count_trigram[str(context)] = 1\n",
    "    \n",
    "    def get_probabilities(context, word):\n",
    "        pass\n",
    "\n",
    "    def P_n_gram_sequence(self, sequence, n): #bigram,unigram,trigram\n",
    "        words = tokenizer.tokenize(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_from_file(path_corpus):\n",
    "    tr_tweet = []    \n",
    "    with open(path_corpus, \"r\") as f_corpus:\n",
    "        for tweet in f_corpus:\n",
    "            tr_tweet += [tweet]\n",
    "    return tr_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_vocabulary(corpus, size):\n",
    "    corpus_words = []\n",
    "    for doc in corpus:\n",
    "        corpus_words += tokenizer.tokenize(doc)\n",
    "    fdist = nltk.FreqDist(corpus_words)\n",
    "    V = [(fdist[key], key) for key in fdist]\n",
    "    V.sort() \n",
    "    V.reverse()\n",
    "    V = V[:size]\n",
    "    V = [word for count, word in V]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus, volcabulary_size):\n",
    "    # Make sure all tweets are in lowercase.\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i]= corpus[i].lower()\n",
    "    # Define vocabulary with the 500 most frequent words.\n",
    "    V = define_vocabulary(corpus, volcabulary_size)\n",
    "    # Rebuild corpus\n",
    "    new_corpus = []\n",
    "    for tweet in corpus:\n",
    "        words = tokenizer.tokenize(tweet)    \n",
    "        new_words = [\"<unk>\" if word not in V else word for word in words] # New lexicon with unknown words\n",
    "        new_corpus.append(new_words)\n",
    "    return new_corpus, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tweet = get_tweets_from_file(\"./mex_train.txt\")\n",
    "tr_tweet, V = preprocess_corpus(tr_tweet, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Language_Model(tr_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lo': 644,\n",
       " 'peor': 37,\n",
       " 'de': 3357,\n",
       " 'todo': 218,\n",
       " 'es': 867,\n",
       " 'que': 3383,\n",
       " 'no': 1824,\n",
       " 'me': 1613,\n",
       " 'dan': 33,\n",
       " 'por': 763,\n",
       " 'un': 697,\n",
       " 'tiempo': 42,\n",
       " 'y': 2266,\n",
       " 'luego': 50,\n",
       " '<unk>': 28023,\n",
       " 'estoy': 241,\n",
       " 'hasta': 199,\n",
       " 'la': 2433,\n",
       " 'verga': 1148,\n",
       " '</s>': 5544,\n",
       " 'a': 2629,\n",
       " 'seas': 23,\n",
       " 'putos': 831,\n",
       " 'minutos': 33,\n",
       " 'despu√©s': 42,\n",
       " 'en': 1191,\n",
       " '3': 63,\n",
       " 'horas': 37,\n",
       " '?': 786,\n",
       " 'm√°s': 314,\n",
       " 'unos': 45,\n",
       " 'mi': 718,\n",
       " '!': 1505,\n",
       " 'madre': 1079,\n",
       " 'ese': 124,\n",
       " 'joto': 209,\n",
       " 'el': 1303,\n",
       " 'marica': 183,\n",
       " 'ex': 29,\n",
       " 'tiene': 123,\n",
       " 'as√≠': 173,\n",
       " 'uno': 58,\n",
       " 'puede': 51,\n",
       " '\"': 652,\n",
       " 'su': 549,\n",
       " 'üòÇ': 377,\n",
       " 'mujer': 55,\n",
       " 'pinche': 296,\n",
       " 'esta': 198,\n",
       " 'se': 969,\n",
       " 'loca': 556,\n",
       " '.': 2774,\n",
       " 'tienen': 104,\n",
       " 'como': 566,\n",
       " 'mejor': 143,\n",
       " 'ustedes': 47,\n",
       " 'si': 662,\n",
       " 'andar': 30,\n",
       " 'pero': 439,\n",
       " 'porque': 230,\n",
       " 'mamar': 61,\n",
       " 'hijos': 109,\n",
       " 'chingada': 70,\n",
       " '@usuario': 1263,\n",
       " 'te': 693,\n",
       " 'digo': 54,\n",
       " 'esa': 126,\n",
       " 'est√°': 189,\n",
       " 'buena': 39,\n",
       " 'odio': 20,\n",
       " 'los': 888,\n",
       " 'üò°': 92,\n",
       " 'tu': 525,\n",
       " 'siempre': 138,\n",
       " 'viene': 20,\n",
       " 'importa': 24,\n",
       " 'digan': 37,\n",
       " 'esos': 97,\n",
       " 'periodistas': 23,\n",
       " 'puta': 277,\n",
       " 'hay': 162,\n",
       " 'selecci√≥n': 21,\n",
       " 'bien': 250,\n",
       " 'ya': 683,\n",
       " '*': 39,\n",
       " 'del': 329,\n",
       " 'üòò': 27,\n",
       " 'con': 770,\n",
       " 'ganas': 68,\n",
       " 'salir': 27,\n",
       " '¬°': 129,\n",
       " 'qu√©': 273,\n",
       " 'esto': 58,\n",
       " 'amo': 42,\n",
       " 'm√©xico': 72,\n",
       " 'gente': 111,\n",
       " '‚Ä¶': 183,\n",
       " '<url>': 56,\n",
       " 'caga': 40,\n",
       " 's√≠': 76,\n",
       " 'todos': 263,\n",
       " 'somos': 20,\n",
       " 'putas': 895,\n",
       " '...': 472,\n",
       " '¬ø': 240,\n",
       " 'tan': 146,\n",
       " 'pendejo': 105,\n",
       " 'estos': 45,\n",
       " 'son': 244,\n",
       " 'pendejos': 60,\n",
       " '2': 63,\n",
       " 'este': 151,\n",
       " ';': 42,\n",
       " 'd': 50,\n",
       " 'una': 502,\n",
       " 'cuenta': 48,\n",
       " 'mis': 152,\n",
       " 'sus': 229,\n",
       " 'cosas': 58,\n",
       " 'quiero': 149,\n",
       " 'hdp': 233,\n",
       " 'las': 628,\n",
       " 'cosa': 24,\n",
       " 'dejar': 30,\n",
       " 'ay': 34,\n",
       " 'üíî': 22,\n",
       " '(': 96,\n",
       " 'jajajajaja': 44,\n",
       " 'pendeja': 77,\n",
       " ')': 104,\n",
       " 'para': 565,\n",
       " 'cada': 70,\n",
       " 'boca': 29,\n",
       " 'esas': 51,\n",
       " 'ser': 181,\n",
       " 've': 48,\n",
       " 'o': 260,\n",
       " 'le': 409,\n",
       " 'novio': 36,\n",
       " 's√≥lo': 50,\n",
       " 'les': 238,\n",
       " 'da': 61,\n",
       " 'miedo': 26,\n",
       " 'saben': 31,\n",
       " 'gusta': 78,\n",
       " 'al': 359,\n",
       " 'menos': 58,\n",
       " 'mas': 94,\n",
       " 'otro': 45,\n",
       " 'ti': 52,\n",
       " 'foto': 39,\n",
       " 'cara': 38,\n",
       " 'maricon': 109,\n",
       " 'tienes': 67,\n",
       " 'chingar': 58,\n",
       " 'luchona': 147,\n",
       " 'hijo': 103,\n",
       " 'mam√°': 78,\n",
       " ':(': 41,\n",
       " 'novia': 27,\n",
       " 'haces': 19,\n",
       " 'yo': 303,\n",
       " 'pues': 88,\n",
       " 'clase': 33,\n",
       " 'tal': 25,\n",
       " 'vez': 112,\n",
       " 'vieja': 22,\n",
       " 'nunca': 60,\n",
       " 'palabra': 31,\n",
       " 'eso': 157,\n",
       " 'sea': 83,\n",
       " 'nos': 132,\n",
       " 'final': 27,\n",
       " 'vale': 163,\n",
       " 'q': 173,\n",
       " 'puto': 160,\n",
       " 'buen': 34,\n",
       " 'jajaja': 107,\n",
       " '‚Äú': 85,\n",
       " '‚Äù': 85,\n",
       " 'jajajaja': 91,\n",
       " 't√∫': 71,\n",
       " 'solo': 147,\n",
       " 'espero': 26,\n",
       " 'hoy': 118,\n",
       " 'mismo': 40,\n",
       " 'todas': 89,\n",
       " 'muy': 118,\n",
       " 'haciendo': 26,\n",
       " 'llorar': 23,\n",
       " 'se√±ora': 27,\n",
       " 'tener': 68,\n",
       " 'ojal√°': 51,\n",
       " 'mundial': 63,\n",
       " 'hondure√±os': 24,\n",
       " 'culo': 64,\n",
       " 'ponen': 27,\n",
       " 'hizo': 23,\n",
       " 'igual': 54,\n",
       " 'pol√≠ticos': 22,\n",
       " 'amigos': 44,\n",
       " 'van': 95,\n",
       " 'ü§î': 48,\n",
       " 'desde': 61,\n",
       " 'vida': 143,\n",
       " 'ando': 27,\n",
       " 'asi': 32,\n",
       " 'fue': 59,\n",
       " 's√∫per': 33,\n",
       " 'trabajo': 37,\n",
       " 'd√≠as': 66,\n",
       " 'buenos': 32,\n",
       " 'semana': 35,\n",
       " 'üôÑ': 71,\n",
       " 'maldito': 21,\n",
       " 'nuevo': 27,\n",
       " 'ella': 38,\n",
       " 'tanto': 59,\n",
       " 'verdad': 67,\n",
       " 'soy': 166,\n",
       " 'andan': 28,\n",
       " 'ir': 66,\n",
       " 'iba': 27,\n",
       " 'vaya': 27,\n",
       " 'twitter': 34,\n",
       " 'lado': 19,\n",
       " 'aunque': 34,\n",
       " 'sabes': 48,\n",
       " 'chingas': 59,\n",
       " 'seguro': 21,\n",
       " 'eres': 144,\n",
       " '10': 32,\n",
       " 'ayer': 22,\n",
       " 'estaba': 51,\n",
       " 'encanta': 45,\n",
       " 'chinga': 43,\n",
       " 'culero': 24,\n",
       " 'gracias': 45,\n",
       " 'ver': 151,\n",
       " 'mucho': 56,\n",
       " 'valgo': 19,\n",
       " 'creo': 42,\n",
       " 'ven': 36,\n",
       " 'neta': 62,\n",
       " 'bonita': 22,\n",
       " 'quien': 83,\n",
       " 'x': 35,\n",
       " 'fuera': 52,\n",
       " 'huevos': 31,\n",
       " 'hace': 109,\n",
       " '1': 37,\n",
       " 'a√±o': 29,\n",
       " 'casa': 56,\n",
       " 'pinches': 155,\n",
       " 'personas': 42,\n",
       " 'valen': 33,\n",
       " '..': 84,\n",
       " 'han': 24,\n",
       " 'vete': 22,\n",
       " 'alv': 59,\n",
       " 'toda': 94,\n",
       " '4': 33,\n",
       " 'sin': 113,\n",
       " 'dinero': 39,\n",
       " 'nada': 144,\n",
       " 'mundo': 64,\n",
       " 'entre': 30,\n",
       " 'madres': 73,\n",
       " 'üò§': 38,\n",
       " ':': 259,\n",
       " 'mujeres': 58,\n",
       " 'volviendo': 24,\n",
       " 'nombre': 19,\n",
       " 'vas': 72,\n",
       " 'bola': 20,\n",
       " 'tengo': 117,\n",
       " 'casi': 22,\n",
       " 'tambi√©n': 68,\n",
       " 'amigo': 39,\n",
       " 'sigue': 34,\n",
       " 'mamando': 66,\n",
       " 'ves': 22,\n",
       " 'siento': 44,\n",
       " 'dice': 76,\n",
       " 'alguien': 91,\n",
       " 'va': 128,\n",
       " 'mal': 71,\n",
       " 'momento': 20,\n",
       " 'he': 49,\n",
       " 'sabe': 47,\n",
       " 'ma√±ana': 63,\n",
       " 'd√≠a': 111,\n",
       " 'Ô∏è': 133,\n",
       " 'mandar': 26,\n",
       " 'oye': 24,\n",
       " \"'\": 73,\n",
       " 'hacen': 51,\n",
       " 'donde': 55,\n",
       " 'lleva': 21,\n",
       " 'rato': 22,\n",
       " 'ahora': 130,\n",
       " 'hecho': 22,\n",
       " 'ni': 208,\n",
       " 'aqu√≠': 60,\n",
       " 'cuando': 281,\n",
       " 'wey': 65,\n",
       " 'e': 46,\n",
       " 'dem√°s': 25,\n",
       " 'dije': 26,\n",
       " 'nueva': 19,\n",
       " 'üòç': 75,\n",
       " 'poca': 37,\n",
       " 'ha': 41,\n",
       " 'pa√≠s': 27,\n",
       " 'decir': 64,\n",
       " 'maric√≥n': 39,\n",
       " 'fotos': 42,\n",
       " 'persona': 31,\n",
       " 'bueno': 58,\n",
       " 'era': 64,\n",
       " 'entonces': 24,\n",
       " 'puro': 27,\n",
       " 'mamen': 35,\n",
       " 'mil': 53,\n",
       " 'nadie': 73,\n",
       " 'tanta': 21,\n",
       " 'algo': 70,\n",
       " 'contigo': 22,\n",
       " 'est√°n': 69,\n",
       " 'parte': 23,\n",
       " 'jajajajajaja': 29,\n",
       " 'saber': 27,\n",
       " 'puedo': 65,\n",
       " 'tarde': 29,\n",
       " 'rico': 41,\n",
       " '‚ù§': 71,\n",
       " 's√©': 72,\n",
       " 'partido': 36,\n",
       " 'mames': 51,\n",
       " 'pu√±al': 25,\n",
       " 'dicen': 48,\n",
       " 'est√°s': 49,\n",
       " 'vergas': 31,\n",
       " 'vali√≥': 26,\n",
       " 'dejen': 28,\n",
       " 'mierda': 140,\n",
       " 'messi': 20,\n",
       " 'trump': 20,\n",
       " 'hacer': 99,\n",
       " 'üçÜ': 26,\n",
       " 'rt': 45,\n",
       " 'lameculos': 21,\n",
       " 'vamos': 39,\n",
       " 'dormir': 34,\n",
       " 'üò©': 37,\n",
       " 'otra': 70,\n",
       " 'necesito': 28,\n",
       " 'puedes': 27,\n",
       " 'llega': 19,\n",
       " 'perra': 39,\n",
       " 'üñï': 34,\n",
       " 'sobre': 25,\n",
       " 'fin': 35,\n",
       " '-': 105,\n",
       " 'ah√≠': 44,\n",
       " 'amiga': 22,\n",
       " 'rica': 40,\n",
       " 'mama': 25,\n",
       " 'pone': 33,\n",
       " 'asco': 27,\n",
       " 'favor': 23,\n",
       " 'quieren': 44,\n",
       " 'hablar': 36,\n",
       " 'vales': 21,\n",
       " 'valer': 45,\n",
       " 'ellos': 36,\n",
       " 'üòå': 22,\n",
       " 'triste': 26,\n",
       " 'c√≥mo': 43,\n",
       " '√©l': 28,\n",
       " 'üé∂': 30,\n",
       " 'valiendo': 27,\n",
       " 'perros': 21,\n",
       " 'a√±os': 63,\n",
       " 'putita': 54,\n",
       " 'quiera': 22,\n",
       " 'antes': 34,\n",
       " 'veo': 41,\n",
       " 're': 27,\n",
       " 'ah': 30,\n",
       " 'pueden': 41,\n",
       " 'ü§£': 36,\n",
       " 'üò≠': 95,\n",
       " '/': 40,\n",
       " 'tarea': 25,\n",
       " 'dijo': 35,\n",
       " 'üò±': 27,\n",
       " 'voy': 152,\n",
       " 'hora': 27,\n",
       " 'noche': 41,\n",
       " 'mientras': 33,\n",
       " 'chinguen': 41,\n",
       " 'estar': 83,\n",
       " 'sale': 23,\n",
       " 'v': 75,\n",
       " 'word': 21,\n",
       " 'a√∫n': 23,\n",
       " 'falta': 39,\n",
       " 'hice': 20,\n",
       " 'm√≠': 36,\n",
       " 'sean': 23,\n",
       " 'quieres': 35,\n",
       " 'pasa': 43,\n",
       " 'perd√≥n': 23,\n",
       " 'creen': 27,\n",
       " 'alg√∫n': 20,\n",
       " 'dios': 31,\n",
       " 'llegar': 20,\n",
       " 'unas': 42,\n",
       " 'mamadas': 29,\n",
       " 'jaja': 52,\n",
       " 'primera': 19,\n",
       " 'entiendo': 21,\n",
       " 'ni√±o': 21,\n",
       " 'seguir': 23,\n",
       " 'vi': 21,\n",
       " 'deja': 50,\n",
       " 'amor': 70,\n",
       " 'viendo': 23,\n",
       " '7': 32,\n",
       " 'dos': 73,\n",
       " 'gran': 25,\n",
       " '√∫nico': 24,\n",
       " 'mira': 37,\n",
       " 'tus': 95,\n",
       " 'risa': 19,\n",
       " 'chingue': 34,\n",
       " 'dar': 31,\n",
       " 'hombre': 37,\n",
       " 'pa': 38,\n",
       " 'volver': 21,\n",
       " 'misma': 20,\n",
       " 'veces': 50,\n",
       " 'pura': 29,\n",
       " 'üí¶': 22,\n",
       " 'üòè': 22,\n",
       " 'üòà': 42,\n",
       " 'bonito': 24,\n",
       " 'siguen': 19,\n",
       " 'gringos': 20,\n",
       " 'hab√≠a': 22,\n",
       " 'pedo': 60,\n",
       " 'pasado': 22,\n",
       " 'feliz': 25,\n",
       " 'hija': 43,\n",
       " 'üôÇ': 19,\n",
       " 'pobre': 19,\n",
       " 'gusto': 24,\n",
       " 'padre': 28,\n",
       " 'estas': 51,\n",
       " 'üòí': 66,\n",
       " 'agua': 25,\n",
       " 'mucha': 20,\n",
       " 'quiere': 63,\n",
       " 'lugar': 28,\n",
       " 'tres': 24,\n",
       " 'tengan': 23,\n",
       " 'hermano': 19,\n",
       " 'xd': 38,\n",
       " 'familia': 22,\n",
       " '|': 21,\n",
       " '5': 32,\n",
       " 'cagan': 27,\n",
       " 'diga': 24,\n",
       " 'doy': 21,\n",
       " 'hubiera': 21,\n",
       " 'trabajar': 19,\n",
       " 'üò†': 25,\n",
       " 'chingo': 34,\n",
       " 'chile': 26,\n",
       " 'üá≤': 21,\n",
       " 'üáΩ': 21,\n",
       " 'todav√≠a': 21,\n",
       " 'hombres': 37,\n",
       " 'üî•': 23,\n",
       " 'video': 22,\n",
       " 'nalgas': 25,\n",
       " 'serio': 19,\n",
       " 'perro': 26,\n",
       " 'putito': 24,\n",
       " 'qui√©n': 43,\n",
       " 'hago': 20,\n",
       " 'üòî': 21,\n",
       " 'gol': 20,\n",
       " 'ratas': 22,\n",
       " 'vayan': 20,\n",
       " 'escuela': 21,\n",
       " 'debe': 21,\n",
       " 'conmigo': 22,\n",
       " 'vuelve': 19,\n",
       " 'üòÜ': 25,\n",
       " 'poder': 22}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Count_unigram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
