{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "# Dataset Original: https://huggingface.co/datasets/bookcorpus\n",
        "#dataset = load_dataset(\"bookcorpus\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL13R9i_zejr",
        "outputId": "0ddd8d83-3dfe-4190-e560-0223efac4e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#dataset.save_to_disk('/content/drive/MyDrive/BookCorpus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQt4HJi8z7Jo",
        "outputId": "ca338e44-3718-4fb8-ef1e-bfa553338761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Cargar el tokenizer de DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "train_data = dataset[\"text\"]\n",
        "\n",
        "\n",
        "with open(\"dump.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for sentence in train_data:\n",
        "        encoded_input = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(encoded_input)\n",
        "        processed_sentence = \" \".join(tokens)\n",
        "\n",
        "        file.write(processed_sentence + \"\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "iKt5FCuJqrpV",
        "outputId": "eae8a6c5-7d2d-4b41-e278-7fbb76383021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nfrom transformers import DistilBertTokenizer\\n\\n# Cargar el tokenizer de DistilBERT\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\ntrain_data = dataset[\"text\"]\\n\\n\\nwith open(\"dump.txt\", \"w\", encoding=\"utf-8\") as file:\\n    for sentence in train_data:\\n        encoded_input = tokenizer.encode(sentence, add_special_tokens=True)\\n        tokens = tokenizer.convert_ids_to_tokens(encoded_input)\\n        processed_sentence = \" \".join(tokens)\\n\\n        file.write(processed_sentence + \"\\n\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import shutil\n",
        "source_path = '/content/dump.txt'\n",
        "destination_path = '/content/drive/MyDrive/dump.txt'\n",
        "shutil.move(source_path, destination_path)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UbWBEbmK6U4g",
        "outputId": "bdc6b663-13f6-4580-e4cf-b05759cf9f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport shutil\\nsource_path = '/content/dump.txt'\\ndestination_path = '/content/drive/MyDrive/dump.txt'\\nshutil.move(source_path, destination_path)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "source_path = '/content/drive/MyDrive/dump.txt'\n",
        "destination_path = '/content/data/dump_copy.txt'\n",
        "shutil.copyfile(source_path, destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t04rlsKjBrVP",
        "outputId": "b490eba6-d54f-4f8b-aa13-b906681e98cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data/dump_copy.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "original_file = '/content/data/dump.txt'\n",
        "reduced_file = '/content/dump_reduced.txt'\n",
        "max_examples = 30000\n",
        "example_count = 0\n",
        "\n",
        "# Leer el archivo original y crear el archivo reducido\n",
        "with open(original_file, 'r', encoding='utf-8') as original:\n",
        "    with open(reduced_file, 'w', encoding='utf-8') as reduced:\n",
        "        for line in original:\n",
        "            if example_count >= max_examples:\n",
        "                break\n",
        "            reduced.write(line)\n",
        "            example_count += 1\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Qaaf32uUivcY",
        "outputId": "283df40d-8ff3-4068-e801-51769cf27520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\noriginal_file = '/content/data/dump.txt'\\nreduced_file = '/content/dump_reduced.txt'\\nmax_examples = 30000\\nexample_count = 0\\n\\n# Leer el archivo original y crear el archivo reducido\\nwith open(original_file, 'r', encoding='utf-8') as original:\\n    with open(reduced_file, 'w', encoding='utf-8') as reduced:\\n        for line in original:\\n            if example_count >= max_examples:\\n                break\\n            reduced.write(line)\\n            example_count += 1\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "knoKhiILzK0X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "975b981f-9fac-45a2-d6b2-3284ddb8b98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers (from -r requirements.txt (line 1))\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitpython==3.1.30 (from -r requirements.txt (line 3))\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.12.2)\n",
            "Collecting tensorboardX==1.8 (from -r requirements.txt (line 5))\n",
            "  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.3/216.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil==5.6.6 (from -r requirements.txt (line 6))\n",
            "  Downloading psutil-5.6.6.tar.gz (447 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.10.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython==3.1.30->-r requirements.txt (line 3))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (3.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14.0->-r requirements.txt (line 4)) (0.40.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython==3.1.30->-r requirements.txt (line 3))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14.0->-r requirements.txt (line 4)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14.0->-r requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14.0->-r requirements.txt (line 4)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=1.14.0->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=1.14.0->-r requirements.txt (line 4)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14.0->-r requirements.txt (line 4)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=1.14.0->-r requirements.txt (line 4)) (3.2.2)\n",
            "Building wheels for collected packages: psutil\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.6.6-cp310-cp310-linux_x86_64.whl size=284374 sha256=ba5c1814914ca9e02aa2e93165d2b33c07ab48d2b4b31120bffc152a75a2265b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/64/a3/407615c080b6bd28ec2f82e431c0623215690c53aa61bff735\n",
            "Successfully built psutil\n",
            "Installing collected packages: tokenizers, tensorboardX, smmap, psutil, gitdb, transformers, gitpython\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.30 psutil-5.6.6 smmap-5.0.0 tensorboardX-1.8 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "D-Ft6NKetpeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install GitPython"
      ],
      "metadata": {
        "id": "TkYUFn47tpjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/binarized_data.py \\\n",
        "    --file_path data/dump.txt \\\n",
        "    --tokenizer_type bert \\\n",
        "    --tokenizer_name bert-base-uncased \\\n",
        "    --dump_file data/binarized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7TULrp7szKN",
        "outputId": "ebc31b5d-3cc7-4b8e-9b05-bba2a848be6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/26/2023 19:31:40 - INFO - __main__ - Loading Tokenizer (bert-base-uncased)\n",
            "05/26/2023 19:31:41 - INFO - __main__ - Loading text from data/dump.txt\n",
            "05/26/2023 19:31:41 - INFO - __main__ - Start encoding\n",
            "05/26/2023 19:31:41 - INFO - __main__ - 30000 examples to process.\n",
            "05/26/2023 19:31:45 - INFO - __main__ - 10000 examples processed. - 4.80s/10000expl\n",
            "05/26/2023 19:31:49 - INFO - __main__ - 20000 examples processed. - 3.09s/10000expl\n",
            "05/26/2023 19:31:52 - INFO - __main__ - 30000 examples processed. - 3.40s/10000expl\n",
            "05/26/2023 19:31:52 - INFO - __main__ - Finished binarization\n",
            "05/26/2023 19:31:52 - INFO - __main__ - 30000 examples processed.\n",
            "05/26/2023 19:31:52 - INFO - __main__ - Dump to data/binarized_text.bert-base-uncased.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/token_counts.py \\\n",
        "    --data_file data/binarized_text.bert-base-uncased.pickle \\\n",
        "    --token_counts_dump data/token_counts.bert-base-uncased.pickle \\\n",
        "    --vocab_size 30522"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkeyKxHPtx57",
        "outputId": "ff1b9be2-f33f-4b84-8fb2-473cee2f8b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/26/2023 19:32:08 - INFO - __main__ - Loading data from data/binarized_text.bert-base-uncased.pickle\n",
            "05/26/2023 19:32:08 - INFO - __main__ - Counting occurrences for MLM.\n",
            "05/26/2023 19:32:08 - INFO - __main__ - Dump to data/token_counts.bert-base-uncased.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --student_type distilbert \\\n",
        "    --student_config training_configs/distilbert-base-uncased.json \\\n",
        "    --teacher_type bert \\\n",
        "    --teacher_name bert-base-uncased \\\n",
        "    --alpha_ce 5.0 --alpha_mlm 2.0 --alpha_cos 1.0 --alpha_clm 0.0 --mlm \\\n",
        "    --freeze_pos_embs \\\n",
        "    --dump_path serialization_dir/my_first_training \\\n",
        "    --data_file data/binarized_text.bert-base-uncased.pickle \\\n",
        "    --token_counts data/token_counts.bert-base-uncased.pickle \\\n",
        "    --force # overwrites the `dump_path` if it already exists."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okOD1ZOvt0-y",
        "outputId": "ee86636f-820a-4e98-e78a-19723e4bf3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-26 19:58:43.212977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/26/2023 19:58:44 - INFO - numexpr.utils - PID: 10526 -  NumExpr defaulting to 2 threads.\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  Initializing GPUs\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - Number of nodes: 1\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - Node ID        : 0\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - Local rank     : 0\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - World size     : 1\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - GPUs per node  : 1\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - Master         : True\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - Multi-node     : False\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - Multi-GPU      : False\n",
            "05/26/2023 19:58:44 - INFO - utils - PID: 10526 -  --- Global rank: 0 - Hostname       : 54d875278b4b\n",
            "05/26/2023 19:58:45 - INFO - utils - PID: 10526 -  Experiment will be dumped and logged in serialization_dir/my_first_training\n",
            "05/26/2023 19:58:45 - INFO - utils - PID: 10526 -  Param: Namespace(force=True, dump_path='serialization_dir/my_first_training', data_file='data/binarized_text.bert-base-uncased.pickle', student_type='distilbert', student_config='training_configs/distilbert-base-uncased.json', student_pretrained_weights=None, teacher_type='bert', teacher_name='bert-base-uncased', temperature=2.0, alpha_ce=5.0, alpha_mlm=2.0, alpha_clm=0.0, alpha_mse=0.0, alpha_cos=1.0, mlm=True, mlm_mask_prop=0.15, word_mask=0.8, word_keep=0.1, word_rand=0.1, mlm_smoothing=0.7, token_counts='data/token_counts.bert-base-uncased.pickle', restrict_ce_to_mask=False, freeze_pos_embs=True, freeze_token_type_embds=False, n_epoch=3, batch_size=5, group_by_size=True, gradient_accumulation_steps=50, warmup_prop=0.05, weight_decay=0.0, learning_rate=0.0005, adam_epsilon=1e-06, max_grad_norm=5.0, initializer_range=0.02, fp16=False, fp16_opt_level='O1', n_gpu=1, local_rank=0, seed=56, log_interval=500, checkpoint_interval=4000, n_nodes=1, node_id=0, global_rank=0, world_size=1, n_gpu_per_node=1, multi_gpu=False, is_master=True, multi_node=False)\n",
            "05/26/2023 19:58:45 - INFO - utils - PID: 10526 -  Special tokens {'unk_token': 100, 'sep_token': 102, 'pad_token': 0, 'cls_token': 101, 'mask_token': 103}\n",
            "05/26/2023 19:58:45 - INFO - utils - PID: 10526 -  Loading data from data/binarized_text.bert-base-uncased.pickle\n",
            "05/26/2023 19:58:45 - INFO - utils - PID: 10526 -  Loading token counts from data/token_counts.bert-base-uncased.pickle (already pre-computed)\n",
            "/content/lm_seqs_dataset.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.token_ids = np.array(data)\n",
            "05/26/2023 19:58:46 - INFO - utils - PID: 10526 -  Splitting 0 too long sequences.\n",
            "/content/lm_seqs_dataset.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.token_ids = np.array(new_tok_ids)\n",
            "05/26/2023 19:58:46 - INFO - utils - PID: 10526 -  Remove 4284 too short (<=11 tokens) sequences.\n",
            "05/26/2023 19:58:46 - INFO - utils - PID: 10526 -  Remove 0 sequences with a high level of unknown tokens (50%).\n",
            "05/26/2023 19:58:46 - INFO - utils - PID: 10526 -  25716 sequences\n",
            "05/26/2023 19:58:46 - INFO - utils - PID: 10526 -  Data loader created.\n",
            "05/26/2023 19:58:46 - INFO - utils - PID: 10526 -  Loading student config from training_configs/distilbert-base-uncased.json\n",
            "05/26/2023 19:58:51 - INFO - utils - PID: 10526 -  Student loaded.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  Teacher loaded from bert-base-uncased.\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  Initializing Distiller\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  Using [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95, 99, 103, 107, 111, 115, 119, 123, 127, 131, 135, 139, 143, 147, 151, 155, 159, 163, 167, 171, 175, 179, 183, 187, 191, 195, 199, 203, 207, 211, 215, 219, 223, 227, 231, 235, 239, 243, 247, 251, 255, 259, 263, 267, 271, 275, 279, 283, 287, 291, 295, 299, 303, 307, 311, 315, 319, 323, 327, 331, 335, 339, 343, 347, 351, 355, 359, 363, 367, 371, 375, 379, 383, 387, 391, 395, 399, 403, 407, 411, 415, 419, 423, 427, 431, 435, 439, 443, 447, 451, 455, 459, 463, 467, 471, 475, 479, 483, 487, 491, 495, 499, 503, 507, 511, inf] as bins for aspect lengths quantization\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  Count of instances per bin: [3974 5463 4675 3709 2803 1921 1247  732  443  296  163  114   74   42\n",
            "   23    7   16    6    4    2    1    1]\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  Using MLM loss for LM step.\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  --- Initializing model optimizer\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  ------ Number of trainable parameters (student): 66592314\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  ------ Number of parameters (student): 66985530\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  --- Initializing Tensorboard\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  Starting training\n",
            "05/26/2023 19:58:54 - INFO - utils - PID: 10526 -  --- Starting epoch 0/2\n",
            "-Iter:  11% 555/5144 [00:23<03:08, 24.30it/s, Last_loss=110.02, Avg_cum_loss=114.83]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "-Iter: 100% 5144/5144 [04:03<00:00, 21.15it/s, Last_loss=33.30, Avg_cum_loss=45.13]\n",
            "05/26/2023 20:02:57 - INFO - utils - PID: 10526 -  --- Ending epoch 0/2\n",
            "05/26/2023 20:02:57 - INFO - utils - PID: 10526 -  25716 sequences have been trained during this epoch.\n",
            "05/26/2023 20:02:58 - INFO - utils - PID: 10526 -  --- Starting epoch 1/2\n",
            "-Iter:  38% 1973/5144 [01:34<02:31, 20.97it/s, Last_loss=23.94, Avg_cum_loss=24.75]Exception ignored in: <generator object tqdm.__iter__ at 0x7fc27ec54510>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1193, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1299, in close\n",
            "    self.display(pos=0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1492, in display\n",
            "    self.sp(self.__str__() if msg is None else msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 347, in print_status\n",
            "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 340, in fp_write\n",
            "    fp.write(str(s))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\", line 127, in inner\n",
            "    return func(*args, **kwargs)\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 324, in <module>\n",
            "    main()\n",
            "  File \"/content/train.py\", line 319, in main\n",
            "    distiller.train()\n",
            "  File \"/content/distiller.py\", line 354, in train\n",
            "    self.step(input_ids=token_ids, attention_mask=attn_mask, lm_labels=lm_labels)\n",
            "  File \"/content/distiller.py\", line 387, in step\n",
            "    teacher_outputs = self.teacher(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1358, in forward\n",
            "    outputs = self.bert(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1020, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 610, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 495, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 425, in forward\n",
            "    self_outputs = self.self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 306, in forward\n",
            "    key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 272, in transpose_for_scores\n",
            "    return x.permute(0, 2, 1, 3)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}